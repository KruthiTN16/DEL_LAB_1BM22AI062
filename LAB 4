import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, learning_rate=0.01):
        # Initialize weights and biases
        self.W1 = np.random.randn(input_size, hidden_size1) * 0.01
        self.b1 = np.zeros((1, hidden_size1))
        self.W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01
        self.b2 = np.zeros((1, hidden_size2))
        self.W3 = np.random.randn(hidden_size2, output_size) * 0.01
        self.b3 = np.zeros((1, output_size))
        self.learning_rate = learning_rate

    def forward(self, X):
        self.Z1 = np.dot(X, self.W1) + self.b1
        self.A1 = self.relu(self.Z1)
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        self.A2 = self.relu(self.Z2)
        self.Z3 = np.dot(self.A2, self.W3) + self.b3
        self.A3 = self.softmax(self.Z3)
        return self.A3

    def relu(self, Z):
        return np.maximum(0, Z)

    def softmax(self, Z):
        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
        return exp_Z / exp_Z.sum(axis=1, keepdims=True)

    def compute_loss(self, Y, Y_hat):
        m = Y.shape[0]
        log_likelihood = -np.log(Y_hat[range(m), Y.argmax(axis=1)])
        loss = np.sum(log_likelihood) / m
        return loss

    def backward(self, X, Y):
        m = Y.shape[0]

        dZ3 = self.A3 - Y
        dW3 = np.dot(self.A2.T, dZ3) / m
        db3 = np.sum(dZ3, axis=0, keepdims=True) / m

        dZ2 = np.dot(dZ3, self.W3.T) * (self.A2 > 0)
        dW2 = np.dot(self.A1.T, dZ2) / m
        db2 = np.sum(dZ2, axis=0, keepdims=True) / m

        dZ1 = np.dot(dZ2, self.W2.T) * (self.A1 > 0)
        dW1 = np.dot(X.T, dZ1) / m
        db1 = np.sum(dZ1, axis=0, keepdims=True) / m

        # Update weights and biases
        self.W3 -= self.learning_rate * dW3
        self.b3 -= self.learning_rate * db3
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1

    def train(self, X, Y, epochs, batch_size, stochastic=False):
        for epoch in range(epochs):
            if stochastic:
                # Stochastic Gradient Descent
                indices = np.arange(X.shape[0])
                np.random.shuffle(indices)
                for start in range(0, len(X), batch_size):
                    end = start + batch_size
                    batch_indices = indices[start:end]
                    X_batch, Y_batch = X[batch_indices], Y[batch_indices]
                    Y_hat = self.forward(X_batch)
                    self.backward(X_batch, Y_batch)
            else:
                # Gradient Descent
                Y_hat = self.forward(X)
                self.backward(X, Y)
                
            if epoch % 100 == 0:
                loss = self.compute_loss(Y, self.forward(X))
                print(f'Epoch {epoch}, Loss: {loss}')

# Example usage:
if __name__ == "__main__":
    # Create some dummy data
    np.random.seed(0)
    X = np.random.rand(100, 10)  # 100 samples, 10 features
    Y = np.zeros((100, 3))  # 100 samples, 3 classes
    Y[np.arange(100), np.random.randint(0, 3, size=100)] = 1  # One-hot encoding

    nn = NeuralNetwork(input_size=10, hidden_size1=5, hidden_size2=5, output_size=3, learning_rate=0.01)
    
    print("Training with Gradient Descent:")
    nn.train(X, Y, epochs=1000, batch_size=32, stochastic=False)

    print("\nTraining with Stochastic Gradient Descent:")
    nn.train(X, Y, epochs=1000, batch_size=32, stochastic=True)
